{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Oaxaca-Blinder Decomposition","text":"<p>The Oaxaca-Blinder decomposition is a statistical method used to explain the difference in outcomes between two groups by decomposing it into:</p> <ol> <li>A part that is \"explained\" by differences in group predictor</li> <li>A part that remains \"unexplained\"</li> </ol> <p>For example, the gender wage gap can be partly \"explained\" by the difference in education and work experience between men and women. The remaining \"unexplained\" part is typically considered discrimination.</p> <p>For a methodological review, see Jann (2008) and Fortin et al. (2011).</p>"},{"location":"#why-use-this-package","title":"Why use this package?","text":"<p>If possible, you should use the Stata package <code>oaxaca</code>, which is the most feature-rich implementation (Jann, 2008). If you can't, existing implementations in R and Python are lacking:</p> <ol> <li>The R <code>oaxaca</code> package does not permit more than 1 categorical variable (discussion)</li> <li>The Python implementation in <code>statsmodels</code> only decomposes into the explained and unexplained part, without a \"detailed decomposition\" into the contribution of each predictor</li> </ol> <p>For industry data science work, these limitations are prohibitive. This package thus fills in the gap by providing:</p> <ol> <li>As table stakes, two-fold and three-fold decomposition, with detailed decomposition for each predictor</li> <li>Multiple ways to deal with the \"omitted base category problem\" (see below)</li> <li>Automatic handling of the case when the two groups don't have a common support. For example, some occupations may only exist in 1975 and not 2025, and vice versa</li> <li>Rich HTML table output</li> </ol> <p>This package does not produce standard error. While possible to add, this feature is deprioritized for the application of industry data science because:</p> <ol> <li>The standard error of the OLS coefficient is often negligible given the number of observation in industry.</li> <li>The standard error of the covariates is 0 given the goal of explaining the difference between two groups in a fixed population. This goal contrasts with that of academics, which is to prove some hypotheses about the world. There, one can imagine a new sample from a superpopulation that has a different covariate distribution.</li> </ol>"},{"location":"#references","title":"References","text":"<p>Fortin, N., Lemieux, T., &amp; Firpo, S. (2011). Decomposition methods in economics. In Handbook of Labor Economics (Vol. 4, pp. 1-102). Elsevier.</p> <p>Jann, B. (2008). A Stata implementation of the Blinder-Oaxaca decomposition. Stata Journal, 8(4), 453-479.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This notebook demonstrates a minimal example.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\n\nfrom oaxaca import Oaxaca\n</pre> import pandas as pd  from oaxaca import Oaxaca <p>We load a sample dataset of Hispanic workers in the Chicago metropolitan area. The goal is to explain the wage gap between native and foreign-born workers. The data is taken from the <code>oaxaca</code> R package.</p> In\u00a0[2]: Copied! <pre>df = pd.read_csv(\"sample_data.csv\")\ndf.head()\n</pre> df = pd.read_csv(\"sample_data.csv\") df.head() Out[2]: age female foreign_born LTHS high_school some_college college advanced_degree education_level ln_real_wage 0 52 0 1 0 1 0 0 0 high_school 2.140066 1 46 1 1 0 1 0 0 0 high_school NaN 2 31 1 1 0 1 0 0 0 high_school 2.499795 3 35 0 1 0 1 0 0 0 high_school 2.708050 4 19 0 0 0 1 0 0 0 high_school 2.079442 <p>We fit the Oaxaca model, using R-style formula to describe the regression.</p> In\u00a0[3]: Copied! <pre>model = Oaxaca().fit(\n    formula=\"exp(ln_real_wage) ~ -1 + age + female + C(education_level)\", data=df, group_variable=\"foreign_born\"\n)\n</pre> model = Oaxaca().fit(     formula=\"exp(ln_real_wage) ~ -1 + age + female + C(education_level)\", data=df, group_variable=\"foreign_born\" ) <p>From the model fit, we can generate two-fold and three-fold decomposition results.</p> In\u00a0[10]: Copied! <pre>twofold_decomposition = model.two_fold(weights={0: 1.0, 1: 0.0})\ntwofold_decomposition\n</pre> twofold_decomposition = model.two_fold(weights={0: 1.0, 1: 0.0}) twofold_decomposition Out[10]: Oaxaca-Blinder Decomposition Results <p>Group Variable: foreign_born | Groups: 0 vs 1 | Direction: 0 - 1 | Weights: Group 0: 1.000, Group 1: 0.000</p> <p>Mean Outcomes: Group 0: 17.5828 | Group 1: 14.5672 | Difference: 3.0156</p> Detailed Variable Contributions VariableExplainedExplained%UnexplainedUnexplained%TotalTotal%age-1.7491-58.0%7.5585250.6%5.8094192.6%female-0.5231-17.3%-1.1653-38.6%-1.6883-56.0%C(education_level)2.454581.4%-3.5599-118.1%-1.1055-36.7%C(education_level)[LTHS]-1.5272-50.6%-1.5892-52.7%-3.1163-103.3%C(education_level)[advanced_degree]0.899329.8%-0.4043-13.4%0.495016.4%C(education_level)[college]0.896529.7%0.23377.8%1.130337.5%C(education_level)[high_school]-0.5832-19.3%-1.2954-43.0%-1.8786-62.3%C(education_level)[some_college]2.769091.8%-0.5048-16.7%2.264275.1%Total0.18226.0%2.833394.0%3.0156100.0% <p> \ud83d\udca1 For programmatic access:                     \u2022 <code>contributions</code> - aggregated categorical variables                     \u2022 <code>detailed_contributions</code> - individual categories with hierarchy                     \u2022 <code>removal_info</code> - per-group removal impact details                 </p> <p>It's worth noting that the <code>age</code> does explain a large portion of the difference. We can zoom into how <code>age</code> differs between native and foreign-born workers with <code>print_x()</code>, and the impact of <code>age</code> on <code>wage</code> with <code>print_ols()</code></p> In\u00a0[5]: Copied! <pre>twofold_decomposition.print_x()\n</pre> twofold_decomposition.print_x() <pre>Difference in X (Predictor Variables) Between Groups\n================================================================================\nGroup Variable: foreign_born\nGroups: 0 (Group 0) vs 1 (Group 1)\nDifference = Group 0 Mean - Group 1 Mean\n\nVariable                                          0 Mean          1 Mean      Difference\n----------------------------------------------------------------------------------------\nage                                              34.0105         40.6359         -6.6254\nfemale                                            0.4808          0.3958          0.0851\nC(education_level)[LTHS]                          0.1185          0.3879         -0.2694\nC(education_level)[advanced_degree]               0.0697          0.0317          0.0380\nC(education_level)[college]                       0.1289          0.0818          0.0471\nC(education_level)[high_school]                   0.2962          0.3641         -0.0679\nC(education_level)[some_college]                  0.3868          0.1346          0.2522\n</pre> In\u00a0[6]: Copied! <pre>twofold_decomposition.print_ols()\n</pre> twofold_decomposition.print_ols() <pre>OLS Regression Results by Group\n============================================================\n\nGroup: 0\n----------------------------------------\nNumber of observations: 287\nR-squared: 0.3268\nMean of dependent variable: 17.5828\nStd of dependent variable: 12.0486\n\nCoefficients:\nVariable                                      Coeff    Std Err        t    P&gt;|t|\n-------------------------------------------------------------\nage                                          0.2640     0.0468    5.637    0.000\nfemale                                      -6.1497     1.1929   -5.155    0.000\nC(education_level)[LTHS]                     5.6689     2.3922    2.370    0.018\nC(education_level)[advanced_degree]         23.6506     2.9415    8.040    0.000\nC(education_level)[college]                 19.0245     2.4208    7.859    0.000\nC(education_level)[high_school]              8.5831     1.9820    4.331    0.000\nC(education_level)[some_college]            10.9798     1.9020    5.773    0.000\n\nGroup: 1\n----------------------------------------\nNumber of observations: 379\nR-squared: 0.3195\nMean of dependent variable: 14.5672\nStd of dependent variable: 8.9085\n\nCoefficients:\nVariable                                      Coeff    Std Err        t    P&gt;|t|\n-------------------------------------------------------------\nage                                          0.0780     0.0310    2.519    0.012\nfemale                                      -3.2055     0.7868   -4.074    0.000\nC(education_level)[LTHS]                     9.7661     1.4673    6.656    0.000\nC(education_level)[advanced_degree]         36.4205     2.4641   14.781    0.000\nC(education_level)[college]                 16.1671     1.9121    8.455    0.000\nC(education_level)[high_school]             12.1407     1.4121    8.598    0.000\nC(education_level)[some_college]            14.7311     1.7540    8.399    0.000\nCoefficient Comparison Between Groups\n================================================================================\nDirection: 0 - 1\n\nVariable                                      Group 0      Group 1   Difference\n-------------------------------------------------------------------------------\nage                                            0.2640       0.0780       0.1860\nfemale                                        -6.1497      -3.2055      -2.9442\nC(education_level)[LTHS]                       5.6689       9.7661      -4.0972\nC(education_level)[advanced_degree]           23.6506      36.4205     -12.7699\nC(education_level)[college]                   19.0245      16.1671       2.8573\nC(education_level)[high_school]                8.5831      12.1407      -3.5576\nC(education_level)[some_college]              10.9798      14.7311      -3.7514\n</pre> In\u00a0[7]: Copied! <pre>threefold_decomposition = model.three_fold()\n</pre> threefold_decomposition = model.three_fold() In\u00a0[8]: Copied! <pre>threefold_decomposition\n</pre> threefold_decomposition Out[8]: Oaxaca-Blinder Decomposition Results (Three-fold) <p>Group Variable: foreign_born | Groups: 0 vs 1 | Direction: 0 - 1</p> <p>Mean Outcomes: Group 0: 17.5828 | Group 1: 14.5672 | Difference: 3.0156</p> Detailed Variable Contributions VariableEndowmentEndowment%CoefficientCoefficient%InteractionInteraction%TotalTotal%age-0.5168-17.1%7.5585250.6%-1.2324-40.9%5.8094192.6%female-0.2727-9.0%-1.1653-38.6%-0.2504-8.3%-1.6883-56.0%C(education_level)2.406079.8%-3.5599-118.1%0.04851.6%-1.1055-36.7%C(education_level)[LTHS]-2.6310-87.2%-1.5892-52.7%1.103836.6%-3.1163-103.3%C(education_level)[advanced_degree]1.384945.9%-0.4043-13.4%-0.4856-16.1%0.495016.4%C(education_level)[college]0.761925.3%0.23377.8%0.13474.5%1.130337.5%C(education_level)[high_school]-0.8249-27.4%-1.2954-43.0%0.24178.0%-1.8786-62.3%C(education_level)[some_college]3.7151123.2%-0.5048-16.7%-0.9461-31.4%2.264275.1%Total1.616553.6%2.833394.0%-1.4343-47.6%3.0156100.0% <p> \ud83d\udca1 For programmatic access:                     \u2022 <code>contributions</code> - aggregated categorical variables                     \u2022 <code>detailed_contributions</code> - individual categories with hierarchy                     \u2022 <code>removal_info</code> - per-group removal impact details                 </p> In\u00a0[\u00a0]: Copied! <pre># Test direction argument by creating decompositions with different directions\n# First, let's check the current direction and total_difference\nprint(\"Current twofold_decomposition:\")\nprint(f\"Direction: {twofold_decomposition.direction}\")\nprint(f\"Total difference: {twofold_decomposition.total_difference}\")\nprint(f\"Groups: {model.groups_}\")\nprint(f\"Group 0 mean_y: {model.group_stats_[model.groups_[0]]['mean_y']}\")\nprint(f\"Group 1 mean_y: {model.group_stats_[model.groups_[1]]['mean_y']}\")\n\n# Calculate what the difference should be for each direction\ngroup_0_mean = model.group_stats_[model.groups_[0]][\"mean_y\"]\ngroup_1_mean = model.group_stats_[model.groups_[1]][\"mean_y\"]\n\nprint(f\"\\nGroup 0 - Group 1: {group_0_mean - group_1_mean:.4f}\")\nprint(f\"Group 1 - Group 0: {group_1_mean - group_0_mean:.4f}\")\n\n# Test threefold decomposition direction\nprint(\"\\nThreefold decomposition:\")\nprint(f\"Direction: {threefold_decomposition.direction}\")\nprint(f\"Total difference: {threefold_decomposition.total_difference}\")\n</pre> # Test direction argument by creating decompositions with different directions # First, let's check the current direction and total_difference print(\"Current twofold_decomposition:\") print(f\"Direction: {twofold_decomposition.direction}\") print(f\"Total difference: {twofold_decomposition.total_difference}\") print(f\"Groups: {model.groups_}\") print(f\"Group 0 mean_y: {model.group_stats_[model.groups_[0]]['mean_y']}\") print(f\"Group 1 mean_y: {model.group_stats_[model.groups_[1]]['mean_y']}\")  # Calculate what the difference should be for each direction group_0_mean = model.group_stats_[model.groups_[0]][\"mean_y\"] group_1_mean = model.group_stats_[model.groups_[1]][\"mean_y\"]  print(f\"\\nGroup 0 - Group 1: {group_0_mean - group_1_mean:.4f}\") print(f\"Group 1 - Group 0: {group_1_mean - group_0_mean:.4f}\")  # Test threefold decomposition direction print(\"\\nThreefold decomposition:\") print(f\"Direction: {threefold_decomposition.direction}\") print(f\"Total difference: {threefold_decomposition.total_difference}\")"},{"location":"getting_started/#two-fold-decomposition","title":"Two-fold decomposition\u00b6","text":"<p>This approach decomposes the wage difference into an explained part vs an unexplained part.</p> <p>The <code>weights</code> argument specifies which group is considered non-discriminated. Here, <code>weights={0: 1.0, 1: 0.0}</code> means that native workers (i.e. those with <code>foreign_born = 0</code>) are the non-discriminated group.</p> <p>We see that the difference in covariates can only explain 6% of the overall difference, suggesting strong evidence of discrimination.</p>"},{"location":"getting_started/#three-fold-decomposition","title":"Three-fold decomposition\u00b6","text":"<p>This approach decomposes the wage gap into three parts: Endowment, Coefficient, and Interaction.</p>"},{"location":"modules/","title":"Modules","text":""},{"location":"modules/#oaxaca.Oaxaca","title":"<code>Oaxaca</code>","text":"<p>Oaxaca-Blinder decomposition for analyzing group differences.</p> <p>The Oaxaca-Blinder decomposition is a statistical method used to explain the difference in outcomes between two groups by decomposing it into explained and unexplained components.</p> <p>Attributes:</p> Name Type Description <code>coef_</code> <p>Dictionary mapping group values to their coefficients (pd.Series).</p> <code>models_</code> <p>Dictionary mapping group values to their fitted OLS models.</p> <code>group_stats_</code> <p>Dictionary mapping group values to their statistics including n_obs, mean_y, mean_X, std_y, and r_squared.</p> <code>group_variable_</code> <p>The name of the column in X that contains the group indicator.</p> <code>groups_</code> <p>The unique groups identified in the data.</p> Source code in <code>src/oaxaca/oaxaca.py</code> <pre><code>class Oaxaca:\n    \"\"\"Oaxaca-Blinder decomposition for analyzing group differences.\n\n    The Oaxaca-Blinder decomposition is a statistical method used to explain\n    the difference in outcomes between two groups by decomposing it into\n    explained and unexplained components.\n\n    Attributes:\n        coef_: Dictionary mapping group values to their coefficients (pd.Series).\n        models_: Dictionary mapping group values to their fitted OLS models.\n        group_stats_: Dictionary mapping group values to their statistics including n_obs,\n            mean_y, mean_X, std_y, and r_squared.\n        group_variable_: The name of the column in X that contains the group indicator.\n        groups_: The unique groups identified in the data.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the Oaxaca-Blinder decomposition model.\"\"\"\n        pass\n\n    def fit(self, formula: str, data: pd.DataFrame, group_variable: str) -&gt; \"Oaxaca\":\n        \"\"\"Fit the Oaxaca-Blinder decomposition model.\n\n        Args:\n            formula: R-style formula for the regression model.\n            data:\n            group_variable: The column that contains the group indicator.\n\n        Returns:\n            The fitted Oaxaca object for method chaining.\n        \"\"\"\n\n        # Store user input\n        self.formula = formula\n        self.group_variable = group_variable\n\n        # Get unique groups\n        self.groups_ = sorted(data[group_variable].unique().tolist())\n        if len(self.groups_) != 2:\n            raise ValueError(\"Group variable must have exactly 2 unique values\")\n\n        # Get rid of missing data\n        data = data.dropna(subset=Formula(self.formula).required_variables)\n        # Ensure common support between two groups\n        data = self._harmonize_common_support(data)\n\n        # Initialize group-specific attributes\n        self.coef_ = {}\n        self.models_ = {}\n        self.group_stats_ = {}\n        self.model_summary_stats_ = {}\n\n        # Fit separate models for each group\n        for group in self.groups_:\n            group_mask = data[group_variable] == group\n            # ensure_full_rank=True since we want the full-rank model for OLS\n            y_group, X_group = Formula(formula).get_model_matrix(\n                data[group_mask], output=\"pandas\", ensure_full_rank=True\n            )\n            self.X_model_spec = X_group.model_spec\n\n            # Check for zero variance columns, which statsmodels.OLS surprisingly just let through silently\n            # errors=\"ignore\" because some models may not have an Intercept\n            variances = X_group.drop(\"Intercept\", axis=1, errors=\"ignore\").var()\n            # Check if any column has zero variance\n            if (variances == 0).any():\n                # Identify the problematic columns\n                zero_variance_cols = variances[variances == 0].index.tolist()\n                X_group = X_group.drop(zero_variance_cols, axis=1)\n                warnings.warn(\n                    f\"Warning: The following columns have zero variance and were removed: {zero_variance_cols}\",\n                    stacklevel=2,\n                )\n\n            model = sm.OLS(y_group, X_group).fit()\n\n            # Store coefficients and stats before removing data since remove_data() corrupts the params index\n            self.coef_[group] = model.params.copy()\n            self.group_stats_[group] = {\n                \"n_obs\": len(y_group),\n                \"mean_y\": float(y_group.mean().iloc[0]),\n                \"mean_X\": X_group.mean(),\n                \"std_y\": float(y_group.std().iloc[0]),\n                \"r_squared\": model.rsquared,\n            }\n\n            # Store model summary statistics before removing data\n            self.model_summary_stats_[group] = {\n                \"bse\": model.bse.copy(),\n                \"tvalues\": model.tvalues.copy(),\n                \"pvalues\": model.pvalues.copy(),\n            }\n\n            # Remove training data from model object to reduce memory usage\n            model.remove_data()\n\n            self.models_[group] = model\n        # Store the model specification for later tying back the dummies to the categorical terms\n        # in the output table\n        # At this point, the two groups have the same categories, so it doesn't matter which one we take\n        del y_group, X_group  # Release memory\n\n        # Check for zero total difference early to avoid division by zero issues\n        group_0, group_1 = self.groups_\n        mean_y_0 = self.group_stats_[group_0][\"mean_y\"]\n        mean_y_1 = self.group_stats_[group_1][\"mean_y\"]\n        total_difference = mean_y_0 - mean_y_1\n\n        if abs(total_difference) &lt; 1e-10:\n            raise ValueError(\n                f\"Total difference between groups is effectively zero ({total_difference:.2e}). \"\n                f\"Group {group_0} mean: {mean_y_0:.6f}, Group {group_1} mean: {mean_y_1:.6f}. \"\n                \"Decomposition is not meaningful when group means are identical.\"\n            )\n\n        # Return self to allow method chaining\n        return self\n\n    def _validate_weights_input(self, weights):\n        if weights is None:\n            raise ValueError(\"Weights must be provided\")\n        if not isinstance(weights, dict):\n            raise TypeError(\"Weights must be a dictionary with group values as keys\")\n        if set(weights.keys()) != set(self.groups_):\n            raise ValueError(f\"Weights keys must match group values: {self.groups_}\")\n        if abs(sum(weights.values()) - 1.0) &gt; 1e-10:\n            raise ValueError(\"Weights must sum to 1.0\")\n\n    def _compute_x_and_coef(self, gu_adjustment: Literal[\"none\", \"unweighted\", \"weighted\"] = \"none\"):\n        \"\"\"Compute E(X) and \u03b2 for both groups, which is all that is needed for both two-fold and three-fold decompositions.\n\n        Args:\n            gu_adjustment: Type of adjustment to apply. Options are:\n                - \"none\": No adjustment (default)\n                - \"unweighted\": Apply Gardeazabal and Ugidos (2004) adjustment. This is equivalent to running the\n                    decomposition leaving out one category at a time, then take the average contributions\n                - \"weighted\": Apply Gardeazabal and Ugidos (2004) adjustment with\n                  weights based on category frequencies. This is equivalent to making the intercept the overall mean outcome,\n                  leaving the coefficients as deviations from the overall mean.\n        \"\"\"\n        if gu_adjustment not in [\"none\", \"unweighted\", \"weighted\"]:\n            raise ValueError(\"gu_adjustment must be one of: 'none', 'unweighted', 'weighted'\")\n\n        group_0, group_1 = self.groups_\n        coef_0 = self.coef_[group_0]\n        coef_1 = self.coef_[group_1]\n        mean_X_0 = self.group_stats_[group_0][\"mean_X\"]\n        mean_X_1 = self.group_stats_[group_1][\"mean_X\"]\n\n        if gu_adjustment != \"none\":\n            mean_X_0 = self.group_stats_all_categories_[group_0][\"mean_X\"]\n            mean_X_1 = self.group_stats_all_categories_[group_1][\"mean_X\"]\n            coef_0 = self._apply_gu_adjustment(coef_0, weight=mean_X_0 if gu_adjustment == \"weighted\" else None)\n            coef_1 = self._apply_gu_adjustment(coef_1, weight=mean_X_1 if gu_adjustment == \"weighted\" else None)\n\n        # Since we potentially manipulated the indices of coef and mean_X, let's check that their indices\n        # are the same, only out of order. pandas won't do so for us\n        if not set(mean_X_0.index) == set(mean_X_1.index) == set(coef_0.index) == set(coef_1.index):\n            raise ValueError(\"Incompatible indices detected\")\n\n        return mean_X_0, mean_X_1, coef_0, coef_1\n\n    def two_fold(\n        self,\n        weights: dict[Any, float],\n        gu_adjustment: Literal[\"none\", \"unweighted\", \"weighted\"] = \"none\",\n        direction: Literal[\"group0 - group1\", \"group1 - group0\"] = \"group0 - group1\",\n    ) -&gt; \"TwoFoldResults\":\n        \"\"\"Perform two-fold decomposition with customizable weights.\n\n        Args:\n            weights: Weights for the non-discriminatory coefficient vector, where keys are\n                the group values and values are the corresponding weights.\n            gu_adjustment: Type of [Gardeazabal and Ugidos (2004)](omitted_base_category_problem.md) adjustment to apply.\n\n                - \"none\": No adjustment\n                - \"unweighted\": Apply unweighted GU adjustment\n                - \"weighted\": Apply weighted GU adjustment\n\n\n            direction: Direction of the decomposition.\n\n                - \"group0 - group1\"\n                - \"group1 - group0\"\n\n        Returns:\n            A new TwoFoldResults object with decomposition results.\n        \"\"\"\n        self._validate_weights_input(weights)\n        if direction not in [\"group0 - group1\", \"group1 - group0\"]:\n            raise ValueError(\"Direction must be either 'group0 - group1' or 'group1 - group0'\")\n\n        group_0, group_1 = self.groups_\n        mean_y_0 = self.group_stats_[group_0][\"mean_y\"]\n        mean_y_1 = self.group_stats_[group_1][\"mean_y\"]\n\n        mean_X_0, mean_X_1, coef_0, coef_1 = self._compute_x_and_coef(gu_adjustment=gu_adjustment)\n        # Calculate non-discriminatory coefficient vector\n        coef_nd = weights[group_0] * coef_0 + weights[group_1] * coef_1\n\n        # Calculate decomposition components\n        total_diff = float(mean_y_0 - mean_y_1)\n        explained = float((mean_X_0 - mean_X_1) @ coef_nd)\n        explained_detailed = (mean_X_0 - mean_X_1) * coef_nd\n        unexplained = float(mean_X_0 @ (coef_0 - coef_nd) + mean_X_1 @ (coef_nd - coef_1))\n        unexplained_detailed = mean_X_0 * (coef_0 - coef_nd) + mean_X_1 * (coef_nd - coef_1)\n        if direction == \"group1 - group0\":\n            total_diff, explained, unexplained = -total_diff, -explained, -unexplained\n            explained_detailed, unexplained_detailed = -explained_detailed, -unexplained_detailed\n        # Get the appropriate categorical mapping based on whether GU adjustment was applied\n        if gu_adjustment != \"none\":\n            categorical_to_dummy = term_dummies_gu_adjusted(self.X_model_spec)\n        else:\n            categorical_to_dummy = term_dummies(self.X_model_spec)\n\n        # Import here to avoid circular imports\n        from .results import TwoFoldResults\n\n        return TwoFoldResults(\n            oaxaca_instance=self,\n            total_difference=total_diff,\n            explained=explained,\n            unexplained=unexplained,\n            explained_detailed=explained_detailed,\n            unexplained_detailed=unexplained_detailed,\n            coef_nondiscriminatory=coef_nd,\n            weights=weights,\n            mean_X_0=mean_X_0,\n            mean_X_1=mean_X_1,\n            categorical_to_dummy=categorical_to_dummy,\n            direction=direction,\n        )\n\n    def three_fold(\n        self,\n        gu_adjustment: Literal[\"none\", \"unweighted\", \"weighted\"] = \"none\",\n        direction: Literal[\"group0 - group1\", \"group1 - group0\"] = \"group0 - group1\",\n    ) -&gt; \"ThreeFoldResults\":\n        \"\"\"Perform three-fold decomposition.\n\n        Args:\n            gu_adjustment: Type of [Gardeazabal and Ugidos (2004)](omitted_base_category_problem.md) adjustment to apply.\n\n                - \"none\": No adjustment\n                - \"unweighted\": Apply unweighted GU adjustment\n                - \"weighted\": Apply weighted GU adjustment\n\n\n            direction: Direction of the decomposition.\n\n                - \"group0 - group1\"\n                - \"group1 - group0\"\n        Returns:\n            A new ThreeFoldResults object with decomposition results.\n        \"\"\"\n        if direction not in [\"group0 - group1\", \"group1 - group0\"]:\n            raise ValueError(\"Direction must be either 'group0 - group1' or 'group1 - group0'\")\n\n        group_0, group_1 = self.groups_\n        mean_y_0 = self.group_stats_[group_0][\"mean_y\"]\n        mean_y_1 = self.group_stats_[group_1][\"mean_y\"]\n        mean_X_0, mean_X_1, coef_0, coef_1 = self._compute_x_and_coef(gu_adjustment=gu_adjustment)\n\n        # Calculate decomposition components\n        total_diff = float(mean_y_0 - mean_y_1)\n\n        # 1. Endowment effect: (X_0 - X_1) * \u03b2_1\n        endowment = float((mean_X_0 - mean_X_1) @ coef_1)\n        endowment_detailed = (mean_X_0 - mean_X_1) * coef_1\n        # 2. Coefficient effect: X_1 * (\u03b2_0 - \u03b2_1)\n        coefficient = float(mean_X_1 @ (coef_0 - coef_1))\n        coefficient_detailed = mean_X_1 * (coef_0 - coef_1)\n        # 3. Interaction effect: (X_0 - X_1) * (\u03b2_0 - \u03b2_1)\n        interaction = float((mean_X_0 - mean_X_1) @ (coef_0 - coef_1))\n        interaction_detailed = (mean_X_0 - mean_X_1) * (coef_0 - coef_1)\n\n        X_diff = mean_X_0 - mean_X_1\n\n        # Apply direction adjustment if needed\n        if direction == \"group1 - group0\":\n            total_diff = -total_diff\n            X_diff = -X_diff\n            endowment = -endowment\n            coefficient = -coefficient\n            interaction = -interaction\n            endowment_detailed = -endowment_detailed\n            coefficient_detailed = -coefficient_detailed\n            interaction_detailed = -interaction_detailed\n\n        # Get the appropriate categorical mapping based on whether GU adjustment was applied\n        if gu_adjustment != \"none\":\n            categorical_to_dummy = term_dummies_gu_adjusted(self.X_model_spec)\n        else:\n            categorical_to_dummy = term_dummies(self.X_model_spec)\n\n        # Import here to avoid circular imports\n        from .results import ThreeFoldResults\n\n        return ThreeFoldResults(\n            oaxaca_instance=self,\n            total_difference=total_diff,\n            endowment=endowment,\n            coefficient=coefficient,\n            interaction=interaction,\n            endowment_detailed=endowment_detailed,\n            coefficient_detailed=coefficient_detailed,\n            interaction_detailed=interaction_detailed,\n            mean_X_0=mean_X_0,\n            mean_X_1=mean_X_1,\n            categorical_to_dummy=categorical_to_dummy,\n            direction=direction,\n        )\n\n    def _harmonize_common_support(self, data: pd.DataFrame):\n        \"\"\"Solve the common support problem by removing rows so that the two groups have the same set of dummies/categories.\"\"\"\n        y = {}\n        X = {}\n        X_model_spec = {}\n        for group in self.groups_:\n            group_mask = data[self.group_variable] == group\n            # ensure_full_rank=False since we're doing data clean up here, not modeling\n            # We don't want the base to interfere with the harmonization\n            # For example, when a base is excluded from a group's model matrix, making it appear to not be exclusive to that group\n            y[group], X[group] = Formula(self.formula).get_model_matrix(\n                data.loc[group_mask, :], output=\"pandas\", ensure_full_rank=False\n            )\n            X_model_spec[group] = X[group].model_spec\n            # Sometimes the user-supplied formula can result in all-0 dummies, such as when they\n            #   specify a categorical level that doesn't exist in the data\n            columns_that_are_all_0 = X[group].columns[(X[group] == 0).all(axis=0)]\n            X[group] = X[group].drop(columns_that_are_all_0, axis=1)\n\n        # Figure out which rows need to be removed to ensure common support\n        self.dummy_removal_result_ = {}\n        self.group_stats_all_categories_ = {}\n        for this, other in zip(self.groups_, self.groups_[::-1]):\n            # Remove dummies that are just all 0\n\n            # Convert to list since pandas can't accept set as index\n            dummies_exclusive_to_this_group = list(set(dummies(X_model_spec[this])) - set(dummies(X_model_spec[other])))\n            rows_to_remove = (\n                X[this].loc[(X[this].loc[:, dummies_exclusive_to_this_group] == 1).any(axis=1), :].index.tolist()\n            )\n\n            # Compute scalar outcomes and share as floats for easier downstream use\n            outcome_pre_removal_val = float(y[this].mean().iloc[0])\n            outcome_post_removal_val = float(y[this].drop(rows_to_remove).mean().iloc[0])\n            # May be NaN if no rows removed; float() preserves NaN\n            outcome_among_removed_val = (\n                float(y[this].loc[rows_to_remove].mean().iloc[0]) if len(rows_to_remove) &gt; 0 else float(\"nan\")\n            )\n            share_removed_val = len(rows_to_remove) / len(y[this])\n            mean_adjustment_val = outcome_pre_removal_val - outcome_post_removal_val\n\n            self.dummy_removal_result_[this] = {\n                \"removed_dummies\": dummies_exclusive_to_this_group,\n                \"rows_to_remove\": rows_to_remove,\n                \"outcome_pre_removal\": outcome_pre_removal_val,\n                \"outcome_post_removal\": outcome_post_removal_val,\n                \"outcome_among_removed\": outcome_among_removed_val,\n                \"share_removed\": share_removed_val,\n                \"mean_adjustment\": mean_adjustment_val,\n            }\n            # In addition to the full-rank model matrix in OLS below,\n            #   calculate the mean of all categories for GU adjustment\n            # We do this opportunistically by using the cleaned data\n            cleaned_X = X[this].drop(rows_to_remove).drop(dummies_exclusive_to_this_group, axis=1)\n            self.group_stats_all_categories_[this] = {\n                \"mean_X\": cleaned_X.mean(),\n            }\n\n        harmonized_data_list = []\n        for group in self.groups_:\n            group_mask = data[self.group_variable] == group\n            data_group = data[group_mask]\n            rows_to_remove = self.dummy_removal_result_[group][\"rows_to_remove\"]\n            # Drop rows by index\n            harmonized_data_list.append(data_group.drop(rows_to_remove, errors=\"ignore\"))\n        return pd.concat(harmonized_data_list, axis=0, ignore_index=True)\n\n    def _apply_gu_adjustment(self, coef: pd.Series, weight: Optional[pd.Series] = None) -&gt; pd.Series:\n        \"\"\"Apply Gardeazabal and Ugidos (2004) adjustment for omitted group problem.\n\n        For each categorical variable:\n        1. Insert coefficient of 0 for omitted base category\n        2. Calculate mean of all dummy coefficients for that categorical variable\n        3. Subtract this mean from each dummy coefficient\n        4. Add this mean to the intercept coefficient\n\n        Args:\n            coef: Original coefficients from OLS regression.\n            weight: If not set, perform the \"classic\" GU adjustment.\n                If set, a useful set of weights is the relative frequency of the categories,\n                which result in the adjusted Intercept equalling the overall mean outcome,\n                and consequently the coef as deviation from the overall mean.\n\n        Returns:\n            Adjusted coefficients.\n        \"\"\"\n\n        new_coef = pd.Series(dtype=float)\n        for term, term_slice in self.X_model_spec.term_slices.items():\n            if term not in term_dummies(self.X_model_spec):\n                # Not a categorical term, so just append the original coef\n                new_coef = pd.concat([new_coef, coef[term_slice]], axis=0)\n            else:\n                # It's a categorical term, so let's apply GU adjustment\n                if len(term.factors) &gt; 1:\n                    raise ValueError(\"We only support single categorical variable, not interaction\")\n                factor = term.factors[0]\n                contrast_state = self.X_model_spec.factor_contrasts[factor]\n                base_category = get_base_category(contrast_state)\n                base_category_column_name = contrast_state.contrasts.get_factor_format(\n                    levels=contrast_state.levels\n                ).format(name=repr(factor), field=base_category)\n\n                # Create extended coefficient series including base category (coefficient = 0)\n                extended_coefs = pd.concat([coef[term_slice], pd.Series({base_category_column_name: 0.0})], axis=0)\n                # The non-full-rank X model-matrix will be named slightly different, e.g.\n                # edu[high_school] instead of edu[T.high_school]\n                # so we reformat the coefficient here to match\n                extended_coefs.index = extended_coefs.index.str.replace(\"[T.\", \"[\", regex=False)\n\n                # Calculate mean of all coefficients (including base = 0)\n                if weight is None:\n                    mean_coef = extended_coefs.mean()\n                else:\n                    # The multiplication of weight and coef relies on pandas index alignment\n                    #    if there are mismatched indices, fill with NaN then drop them\n                    mean_coef = weight.mul(extended_coefs, fill_value=None).dropna().sum()\n\n                # Adjust the coefficients, including the intercept\n                extended_coefs -= mean_coef\n                new_coef = pd.concat([new_coef, extended_coefs], axis=0)\n                if \"Intercept\" in new_coef.index:\n                    new_coef[\"Intercept\"] += mean_coef\n\n        # Ensure return type is Series (pd.concat can infer Series | DataFrame)\n        return pd.Series(new_coef)\n</code></pre>"},{"location":"modules/#oaxaca.Oaxaca.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Oaxaca-Blinder decomposition model.</p> Source code in <code>src/oaxaca/oaxaca.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the Oaxaca-Blinder decomposition model.\"\"\"\n    pass\n</code></pre>"},{"location":"modules/#oaxaca.Oaxaca.fit","title":"<code>fit(formula, data, group_variable)</code>","text":"<p>Fit the Oaxaca-Blinder decomposition model.</p> <p>Parameters:</p> Name Type Description Default <code>formula</code> <code>str</code> <p>R-style formula for the regression model.</p> required <code>data</code> <code>DataFrame</code> required <code>group_variable</code> <code>str</code> <p>The column that contains the group indicator.</p> required <p>Returns:</p> Type Description <code>Oaxaca</code> <p>The fitted Oaxaca object for method chaining.</p> Source code in <code>src/oaxaca/oaxaca.py</code> <pre><code>def fit(self, formula: str, data: pd.DataFrame, group_variable: str) -&gt; \"Oaxaca\":\n    \"\"\"Fit the Oaxaca-Blinder decomposition model.\n\n    Args:\n        formula: R-style formula for the regression model.\n        data:\n        group_variable: The column that contains the group indicator.\n\n    Returns:\n        The fitted Oaxaca object for method chaining.\n    \"\"\"\n\n    # Store user input\n    self.formula = formula\n    self.group_variable = group_variable\n\n    # Get unique groups\n    self.groups_ = sorted(data[group_variable].unique().tolist())\n    if len(self.groups_) != 2:\n        raise ValueError(\"Group variable must have exactly 2 unique values\")\n\n    # Get rid of missing data\n    data = data.dropna(subset=Formula(self.formula).required_variables)\n    # Ensure common support between two groups\n    data = self._harmonize_common_support(data)\n\n    # Initialize group-specific attributes\n    self.coef_ = {}\n    self.models_ = {}\n    self.group_stats_ = {}\n    self.model_summary_stats_ = {}\n\n    # Fit separate models for each group\n    for group in self.groups_:\n        group_mask = data[group_variable] == group\n        # ensure_full_rank=True since we want the full-rank model for OLS\n        y_group, X_group = Formula(formula).get_model_matrix(\n            data[group_mask], output=\"pandas\", ensure_full_rank=True\n        )\n        self.X_model_spec = X_group.model_spec\n\n        # Check for zero variance columns, which statsmodels.OLS surprisingly just let through silently\n        # errors=\"ignore\" because some models may not have an Intercept\n        variances = X_group.drop(\"Intercept\", axis=1, errors=\"ignore\").var()\n        # Check if any column has zero variance\n        if (variances == 0).any():\n            # Identify the problematic columns\n            zero_variance_cols = variances[variances == 0].index.tolist()\n            X_group = X_group.drop(zero_variance_cols, axis=1)\n            warnings.warn(\n                f\"Warning: The following columns have zero variance and were removed: {zero_variance_cols}\",\n                stacklevel=2,\n            )\n\n        model = sm.OLS(y_group, X_group).fit()\n\n        # Store coefficients and stats before removing data since remove_data() corrupts the params index\n        self.coef_[group] = model.params.copy()\n        self.group_stats_[group] = {\n            \"n_obs\": len(y_group),\n            \"mean_y\": float(y_group.mean().iloc[0]),\n            \"mean_X\": X_group.mean(),\n            \"std_y\": float(y_group.std().iloc[0]),\n            \"r_squared\": model.rsquared,\n        }\n\n        # Store model summary statistics before removing data\n        self.model_summary_stats_[group] = {\n            \"bse\": model.bse.copy(),\n            \"tvalues\": model.tvalues.copy(),\n            \"pvalues\": model.pvalues.copy(),\n        }\n\n        # Remove training data from model object to reduce memory usage\n        model.remove_data()\n\n        self.models_[group] = model\n    # Store the model specification for later tying back the dummies to the categorical terms\n    # in the output table\n    # At this point, the two groups have the same categories, so it doesn't matter which one we take\n    del y_group, X_group  # Release memory\n\n    # Check for zero total difference early to avoid division by zero issues\n    group_0, group_1 = self.groups_\n    mean_y_0 = self.group_stats_[group_0][\"mean_y\"]\n    mean_y_1 = self.group_stats_[group_1][\"mean_y\"]\n    total_difference = mean_y_0 - mean_y_1\n\n    if abs(total_difference) &lt; 1e-10:\n        raise ValueError(\n            f\"Total difference between groups is effectively zero ({total_difference:.2e}). \"\n            f\"Group {group_0} mean: {mean_y_0:.6f}, Group {group_1} mean: {mean_y_1:.6f}. \"\n            \"Decomposition is not meaningful when group means are identical.\"\n        )\n\n    # Return self to allow method chaining\n    return self\n</code></pre>"},{"location":"modules/#oaxaca.Oaxaca.three_fold","title":"<code>three_fold(gu_adjustment='none', direction='group0 - group1')</code>","text":"<p>Perform three-fold decomposition.</p> <p>Parameters:</p> Name Type Description Default <code>gu_adjustment</code> <code>Literal['none', 'unweighted', 'weighted']</code> <p>Type of Gardeazabal and Ugidos (2004) adjustment to apply.</p> <ul> <li>\"none\": No adjustment</li> <li>\"unweighted\": Apply unweighted GU adjustment</li> <li>\"weighted\": Apply weighted GU adjustment</li> </ul> <code>'none'</code> <code>direction</code> <code>Literal['group0 - group1', 'group1 - group0']</code> <p>Direction of the decomposition.</p> <ul> <li>\"group0 - group1\"</li> <li>\"group1 - group0\"</li> </ul> <code>'group0 - group1'</code> <p>Returns:     A new ThreeFoldResults object with decomposition results.</p> Source code in <code>src/oaxaca/oaxaca.py</code> <pre><code>def three_fold(\n    self,\n    gu_adjustment: Literal[\"none\", \"unweighted\", \"weighted\"] = \"none\",\n    direction: Literal[\"group0 - group1\", \"group1 - group0\"] = \"group0 - group1\",\n) -&gt; \"ThreeFoldResults\":\n    \"\"\"Perform three-fold decomposition.\n\n    Args:\n        gu_adjustment: Type of [Gardeazabal and Ugidos (2004)](omitted_base_category_problem.md) adjustment to apply.\n\n            - \"none\": No adjustment\n            - \"unweighted\": Apply unweighted GU adjustment\n            - \"weighted\": Apply weighted GU adjustment\n\n\n        direction: Direction of the decomposition.\n\n            - \"group0 - group1\"\n            - \"group1 - group0\"\n    Returns:\n        A new ThreeFoldResults object with decomposition results.\n    \"\"\"\n    if direction not in [\"group0 - group1\", \"group1 - group0\"]:\n        raise ValueError(\"Direction must be either 'group0 - group1' or 'group1 - group0'\")\n\n    group_0, group_1 = self.groups_\n    mean_y_0 = self.group_stats_[group_0][\"mean_y\"]\n    mean_y_1 = self.group_stats_[group_1][\"mean_y\"]\n    mean_X_0, mean_X_1, coef_0, coef_1 = self._compute_x_and_coef(gu_adjustment=gu_adjustment)\n\n    # Calculate decomposition components\n    total_diff = float(mean_y_0 - mean_y_1)\n\n    # 1. Endowment effect: (X_0 - X_1) * \u03b2_1\n    endowment = float((mean_X_0 - mean_X_1) @ coef_1)\n    endowment_detailed = (mean_X_0 - mean_X_1) * coef_1\n    # 2. Coefficient effect: X_1 * (\u03b2_0 - \u03b2_1)\n    coefficient = float(mean_X_1 @ (coef_0 - coef_1))\n    coefficient_detailed = mean_X_1 * (coef_0 - coef_1)\n    # 3. Interaction effect: (X_0 - X_1) * (\u03b2_0 - \u03b2_1)\n    interaction = float((mean_X_0 - mean_X_1) @ (coef_0 - coef_1))\n    interaction_detailed = (mean_X_0 - mean_X_1) * (coef_0 - coef_1)\n\n    X_diff = mean_X_0 - mean_X_1\n\n    # Apply direction adjustment if needed\n    if direction == \"group1 - group0\":\n        total_diff = -total_diff\n        X_diff = -X_diff\n        endowment = -endowment\n        coefficient = -coefficient\n        interaction = -interaction\n        endowment_detailed = -endowment_detailed\n        coefficient_detailed = -coefficient_detailed\n        interaction_detailed = -interaction_detailed\n\n    # Get the appropriate categorical mapping based on whether GU adjustment was applied\n    if gu_adjustment != \"none\":\n        categorical_to_dummy = term_dummies_gu_adjusted(self.X_model_spec)\n    else:\n        categorical_to_dummy = term_dummies(self.X_model_spec)\n\n    # Import here to avoid circular imports\n    from .results import ThreeFoldResults\n\n    return ThreeFoldResults(\n        oaxaca_instance=self,\n        total_difference=total_diff,\n        endowment=endowment,\n        coefficient=coefficient,\n        interaction=interaction,\n        endowment_detailed=endowment_detailed,\n        coefficient_detailed=coefficient_detailed,\n        interaction_detailed=interaction_detailed,\n        mean_X_0=mean_X_0,\n        mean_X_1=mean_X_1,\n        categorical_to_dummy=categorical_to_dummy,\n        direction=direction,\n    )\n</code></pre>"},{"location":"modules/#oaxaca.Oaxaca.two_fold","title":"<code>two_fold(weights, gu_adjustment='none', direction='group0 - group1')</code>","text":"<p>Perform two-fold decomposition with customizable weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>dict[Any, float]</code> <p>Weights for the non-discriminatory coefficient vector, where keys are the group values and values are the corresponding weights.</p> required <code>gu_adjustment</code> <code>Literal['none', 'unweighted', 'weighted']</code> <p>Type of Gardeazabal and Ugidos (2004) adjustment to apply.</p> <ul> <li>\"none\": No adjustment</li> <li>\"unweighted\": Apply unweighted GU adjustment</li> <li>\"weighted\": Apply weighted GU adjustment</li> </ul> <code>'none'</code> <code>direction</code> <code>Literal['group0 - group1', 'group1 - group0']</code> <p>Direction of the decomposition.</p> <ul> <li>\"group0 - group1\"</li> <li>\"group1 - group0\"</li> </ul> <code>'group0 - group1'</code> <p>Returns:</p> Type Description <code>TwoFoldResults</code> <p>A new TwoFoldResults object with decomposition results.</p> Source code in <code>src/oaxaca/oaxaca.py</code> <pre><code>def two_fold(\n    self,\n    weights: dict[Any, float],\n    gu_adjustment: Literal[\"none\", \"unweighted\", \"weighted\"] = \"none\",\n    direction: Literal[\"group0 - group1\", \"group1 - group0\"] = \"group0 - group1\",\n) -&gt; \"TwoFoldResults\":\n    \"\"\"Perform two-fold decomposition with customizable weights.\n\n    Args:\n        weights: Weights for the non-discriminatory coefficient vector, where keys are\n            the group values and values are the corresponding weights.\n        gu_adjustment: Type of [Gardeazabal and Ugidos (2004)](omitted_base_category_problem.md) adjustment to apply.\n\n            - \"none\": No adjustment\n            - \"unweighted\": Apply unweighted GU adjustment\n            - \"weighted\": Apply weighted GU adjustment\n\n\n        direction: Direction of the decomposition.\n\n            - \"group0 - group1\"\n            - \"group1 - group0\"\n\n    Returns:\n        A new TwoFoldResults object with decomposition results.\n    \"\"\"\n    self._validate_weights_input(weights)\n    if direction not in [\"group0 - group1\", \"group1 - group0\"]:\n        raise ValueError(\"Direction must be either 'group0 - group1' or 'group1 - group0'\")\n\n    group_0, group_1 = self.groups_\n    mean_y_0 = self.group_stats_[group_0][\"mean_y\"]\n    mean_y_1 = self.group_stats_[group_1][\"mean_y\"]\n\n    mean_X_0, mean_X_1, coef_0, coef_1 = self._compute_x_and_coef(gu_adjustment=gu_adjustment)\n    # Calculate non-discriminatory coefficient vector\n    coef_nd = weights[group_0] * coef_0 + weights[group_1] * coef_1\n\n    # Calculate decomposition components\n    total_diff = float(mean_y_0 - mean_y_1)\n    explained = float((mean_X_0 - mean_X_1) @ coef_nd)\n    explained_detailed = (mean_X_0 - mean_X_1) * coef_nd\n    unexplained = float(mean_X_0 @ (coef_0 - coef_nd) + mean_X_1 @ (coef_nd - coef_1))\n    unexplained_detailed = mean_X_0 * (coef_0 - coef_nd) + mean_X_1 * (coef_nd - coef_1)\n    if direction == \"group1 - group0\":\n        total_diff, explained, unexplained = -total_diff, -explained, -unexplained\n        explained_detailed, unexplained_detailed = -explained_detailed, -unexplained_detailed\n    # Get the appropriate categorical mapping based on whether GU adjustment was applied\n    if gu_adjustment != \"none\":\n        categorical_to_dummy = term_dummies_gu_adjusted(self.X_model_spec)\n    else:\n        categorical_to_dummy = term_dummies(self.X_model_spec)\n\n    # Import here to avoid circular imports\n    from .results import TwoFoldResults\n\n    return TwoFoldResults(\n        oaxaca_instance=self,\n        total_difference=total_diff,\n        explained=explained,\n        unexplained=unexplained,\n        explained_detailed=explained_detailed,\n        unexplained_detailed=unexplained_detailed,\n        coef_nondiscriminatory=coef_nd,\n        weights=weights,\n        mean_X_0=mean_X_0,\n        mean_X_1=mean_X_1,\n        categorical_to_dummy=categorical_to_dummy,\n        direction=direction,\n    )\n</code></pre>"},{"location":"omitted_base_category_problem/","title":"The omitted base category problem","text":"<p>The choice of the omitted base category in a regression affects the value of the other coefficients, which in turn affects the contribution of a predictor. This has the disturbing implication that, depending on the analyst's choice of the omitted base category, the same predictor may appear more or less important.</p> <p>This is a well-known problem in the literature (see Jann, 2008, p. 9 for a discussion). Some important implications:</p> <ol> <li>The total <code>explained</code> and <code>unexplained</code> parts are invariant to the omitted group</li> <li>The <code>explained</code> part of a categorical variable is invariant. The <code>unexplained</code> part is NOT invariant.</li> <li>Both the <code>explained</code> and <code>unexplained</code> parts of the dummy within the categorical variable is NOT invariant.</li> </ol> <p>The package offers three solutions (via the <code>gu_adjustment</code> option):</p> <ol> <li>Not do anything. The analyst can choose a business-relevant category to omit (conveniently via R-style formula). The intercept then represents the mean of omitted category, and the remaining dummy coefficients are deviation from this mean.</li> <li>Restrict the coefficients for the single categories to sum to zero. The intercept then represents the mean of the categories. This is the common approach in the academic literature, proposed by Gardeazabal and Ugidos (2004) and Yun (2005).</li> <li>Restrict the coefficients for the single categories to weighted sum to zero. The intercept then represents the overall mean. This probably makes the most sense in an industry data science application.</li> </ol>"},{"location":"omitted_base_category_problem/#proof","title":"Proof","text":"<p>Following the notation in Jann (2008), we have the two-fold decomposition formula:</p> <p>\\(m_A - m_B = (\\bar{X}_{A} - \\bar{X}_{B}) \\, \\beta_{A k}\\)</p> <p>\\(\\sum_{k \\neq 1} (\\bar{X}_{A k} - \\bar{X}_{B k}) \\, \\beta_{A k}\\)</p> \\[ = \\sum_{k \\neq 3} (\\bar{X}_{A k} - \\bar{X}_{B k}) \\, \\beta'_{A k} \\] \\[ = \\sum_{k \\neq 3} (\\bar{X}_{A k} - \\bar{X}_{B k}) (\\beta_{A k} - \\beta_{A 3}) \\] \\[ = \\sum_{k \\neq 3} (\\bar{X}_{A k} - \\bar{X}_{B k}) \\, \\beta_{A k} - \\left( \\sum_{k \\neq 3} (\\bar{X}_{A k} - \\bar{X}_{B k}) \\right) \\beta_{A 3} \\] \\[ = (\\bar{X}_{A 1} - \\bar{X}_{B 1}) \\beta_{A 1} + \\beta_{A 3} \\sum_{k \\neq 3} (\\bar{X}_{A k} - \\bar{X}_{B k}) \\, \\beta_{A k} \\]"},{"location":"omitted_base_category_problem/#references","title":"References","text":"<p>Jann, B. (2008). A Stata implementation of the Blinder-Oaxaca decomposition. Stata Journal, 8(4), 453-479.</p> <p>Gardeazabal, J., &amp; Ugidos, A. (2004). More on identification in detailed wage decompositions. The Review of Economics and Statistics, 86(4), 1034\u20131036.</p> <p>Yun, M.-S. (2005). A simple solution to the identification problem in detailed wage decompositions. Economic Inquiry, 43(4), 766\u2013772.</p>"}]}